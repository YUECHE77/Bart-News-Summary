{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:38:09.852419800Z",
     "start_time": "2024-06-26T03:37:54.374836900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import lawrouge\n",
    "\n",
    "from typing import Dict\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, \n",
    "                          BartForConditionalGeneration)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install lawrouge\n",
    "# !pip install datasets\n",
    "# !pip install accelerate -U\n",
    "# !pip install transformers[torch]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7405facb9383839"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_data(source_path, target_path, output_file, use_partial=None):\n",
    "    with open(source_path, 'r', encoding='utf-8') as src_f, open(target_path, 'r', encoding='utf-8') as tar_f:\n",
    "        src_line = src_f.readlines()\n",
    "        tar_line = tar_f.readlines()\n",
    "        \n",
    "        if use_partial is not None:\n",
    "            src_line = src_line[:use_partial]\n",
    "            tar_line = tar_line[:use_partial]\n",
    "    \n",
    "    assert len(src_line) == len(tar_line), 'Source and target files must have the same number of lines'\n",
    "    \n",
    "    all_data = []\n",
    "    for src, tar in zip(src_line, tar_line):\n",
    "        data = {}\n",
    "        data['source'] = src.strip()\n",
    "        data['target'] = tar.strip()\n",
    "        \n",
    "        all_data.append(data)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sample in all_data:\n",
    "            json_sample = json.dumps(sample, ensure_ascii=False)\n",
    "            f.write(json_sample)\n",
    "            f.write('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:39:56.241955Z",
     "start_time": "2024-06-26T03:39:56.234455200Z"
    }
   },
   "id": "5cce2580ff3a085e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "get_data('dataset/LCSTS/train.src.txt', 'dataset/LCSTS/train.tgt.txt', 'dataset/LCSTS/train_data.json', use_partial=50000)\n",
    "get_data('dataset/LCSTS/valid.src.txt', 'dataset/LCSTS/valid.tgt.txt', 'dataset/LCSTS/val_data.json')\n",
    "get_data('dataset/LCSTS/test.src.txt', 'dataset/LCSTS/test.tgt.txt', 'dataset/LCSTS/test_data.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:42:31.865803600Z",
     "start_time": "2024-06-26T03:42:28.082296800Z"
    }
   },
   "id": "cbbca1879b9d42b7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_dataset = load_dataset('json', data_files='dataset/LCSTS/train_data.json')\n",
    "test_dataset = load_dataset('json', data_files='dataset/LCSTS/test_data.json')\n",
    "val_dataset = load_dataset('json', data_files='dataset/LCSTS/val_data.json')\n",
    "\n",
    "# 加载tokenizer,中文bart使用bert的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:56:33.177564700Z",
     "start_time": "2024-06-26T03:56:30.168307Z"
    }
   },
   "id": "d676f2d3dbd9eebf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10666 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf69052a7f984148a53b99e131a36481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1106 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21bbd2130ce04cf6b5db4c3b6bdc7460"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(example):\n",
    "    return {\n",
    "        'document': example['source'],\n",
    "        'summary': example['target'],\n",
    "        'id': '0'\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset['train'].map(flatten, remove_columns=['source', 'target'])\n",
    "test_dataset = test_dataset['train'].map(flatten, remove_columns=['source', 'target'])\n",
    "val_dataset = val_dataset['train'].map(flatten, remove_columns=['source', 'target'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:56:36.382588200Z",
     "start_time": "2024-06-26T03:56:35.589033900Z"
    }
   },
   "id": "2f3dd2b5069e230e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 1106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 10666\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = datasets.DatasetDict({\"train\":train_dataset, \"validation\": val_dataset, \"test\":test_dataset})\n",
    "\n",
    "print(datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:58:24.287412200Z",
     "start_time": "2024-06-26T03:58:24.275554Z"
    }
   },
   "id": "86622da9bd11da94"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': '今天有传在北京某小区，一光头明星因吸毒被捕的消息。下午北京警方官方微博发布声明通报情况，证实该明星为李代沫。李代沫伙同另外6人，于17日晚在北京朝阳区三里屯某小区的暂住地内吸食毒品，6人全部被警方抓获，且当事人对犯案实施供认不讳。', 'summary': '北京警方确认李代沫吸毒被捕(图)', 'id': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"validation\"][7])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T03:58:47.908384Z",
     "start_time": "2024-06-26T03:58:47.897889Z"
    }
   },
   "id": "f07ab05358c582a6"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "max_input_length = 256 # 最大输入长度\n",
    "max_target_length = 128 # 最大输出长度\n",
    "\n",
    "learning_rate = 1e-4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:00:50.894685100Z",
     "start_time": "2024-06-26T04:00:50.880154300Z"
    }
   },
   "id": "fa92819ab2c7f227"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    document作为输入，summary作为标签\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(examples[\"document\"], max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:02:00.340138400Z",
     "start_time": "2024-06-26T04:02:00.322556300Z"
    }
   },
   "id": "df4205c5087dfecd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61287ffd509844ac8569601de79c2acd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\EE541\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1106 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce916a37fd1442118840577e06f63d46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10666 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d95ecdc3fa9a4b17806363ea919c8d0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\", \"id\"])\n",
    "\n",
    "print(tokenized_datasets[\"train\"][7].keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:03:05.995167300Z",
     "start_time": "2024-06-26T04:02:08.974878600Z"
    }
   },
   "id": "5c72213a9cf7ba55"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 50000\n    })\n    validation: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 1106\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 10666\n    })\n})"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:03:37.000759500Z",
     "start_time": "2024-06-26T04:03:36.988668200Z"
    }
   },
   "id": "c0ca88d94bedf1dd"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 21490, 10936, 6188, 5066, 8938, 6411, 12688, 8500, 3565, 6188, 5066, 8938, 11658, 8403, 23137, 17598, 6427, 6399, 9191, 5080, 12495, 15206, 15350, 6507, 21545, 5144, 16423, 14502, 10892, 9206, 10765, 23397, 8909, 5241, 5175, 20447, 25818, 8344, 8939, 5028, 6188, 5066, 8938, 16306, 4896, 8510, 33345, 6432, 5080, 12495, 15206, 15350, 6507, 15134, 5144, 6544, 5834, 8335, 25818, 9053, 17202, 17205, 15206, 15350, 6507, 6436, 8351, 12688, 23236, 5232, 8344, 20179, 3566, 5080, 12495, 15206, 15350, 6507, 5122, 2483, 16306, 4905, 10892, 2484, 6350, 15206, 15350, 11658, 8403, 23137, 6067, 11541, 25818, 11274, 5965, 4906, 5493, 5959, 5028, 15245, 5040, 5965, 19631, 3566, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [101, 6188, 5066, 8938, 16306, 4896, 8510, 5080, 12495, 15206, 15350, 6507, 6436, 8351, 12688, 23236, 5232, 8344, 20179, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][7])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:04:23.775268300Z",
     "start_time": "2024-06-26T04:04:23.753438100Z"
    }
   },
   "id": "afd72f4fcfe3d87a"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def collate_fn(features: Dict):\n",
    " \n",
    "    batch_input_ids = torch.tensor([feature[\"input_ids\"] for feature in features], dtype=torch.long)\n",
    "    batch_attention_mask = torch.tensor([feature[\"attention_mask\"] for feature in features], dtype=torch.long)\n",
    "    batch_labels = torch.tensor([feature[\"labels\"] for feature in features], dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": batch_input_ids,\n",
    "        \"attention_mask\": batch_attention_mask,\n",
    "        \"labels\": batch_labels\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:14:04.940984300Z",
     "start_time": "2024-06-26T04:14:04.930062200Z"
    }
   },
   "id": "e7634620c5f740a1"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# 构建DataLoader来验证collate_fn\n",
    "dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=False, batch_size=4, collate_fn=collate_fn)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['labels'].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:14:52.145987500Z",
     "start_time": "2024-06-26T04:14:52.098584700Z"
    }
   },
   "id": "ea466c61d47f6aae"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/561M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc434fa322e74af5b9195504fb0e32d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:16:24.117502500Z",
     "start_time": "2024-06-26T04:15:09.699123600Z"
    }
   },
   "id": "51b48b91b2168ee"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqLMOutput(loss=tensor(13.2304, grad_fn=<NllLossBackward0>), logits=tensor([[[-3.1715, -2.9859, -2.2114,  ..., -3.7780, -2.5146, -1.5349],\n",
      "         [-7.3804, -7.1388, -7.0963,  ..., -5.5368, -2.8200, -3.3821],\n",
      "         [-7.9509, -8.5210, -8.5947,  ..., -7.5362, -5.7510, -6.2699],\n",
      "         ...,\n",
      "         [-2.6390, -3.2971, -2.9836,  ..., -1.0758, -0.9355, -0.3422],\n",
      "         [-2.5106, -3.2141, -2.9439,  ..., -0.9418, -0.9477, -0.3904],\n",
      "         [-2.6441, -3.3174, -3.0127,  ..., -1.0721, -0.8910, -0.3352]],\n",
      "\n",
      "        [[-4.0979, -3.8783, -3.1265,  ..., -4.7564, -3.9721, -2.7715],\n",
      "         [-6.5775, -6.4944, -6.8897,  ..., -3.4292, -1.3200, -3.5792],\n",
      "         [-6.4953, -6.8784, -6.9473,  ..., -3.1584, -2.0840, -2.6658],\n",
      "         ...,\n",
      "         [-2.4619, -3.1553, -2.7983,  ..., -0.2950, -1.6573, -0.4402],\n",
      "         [-2.4104, -3.0951, -2.8055,  ..., -0.1244, -1.6583, -0.5303],\n",
      "         [-2.4324, -3.1394, -2.8088,  ..., -0.2124, -1.5722, -0.3626]],\n",
      "\n",
      "        [[-4.2039, -3.7437, -3.4780,  ..., -4.4089, -3.4337, -3.6283],\n",
      "         [-6.7891, -6.7869, -7.2024,  ..., -5.7114, -3.8725, -2.2751],\n",
      "         [-6.1372, -6.3245, -6.8426,  ..., -7.1014, -2.8262, -2.6729],\n",
      "         ...,\n",
      "         [-3.4423, -3.2347, -2.7785,  ..., -2.2683, -2.7473, -1.1928],\n",
      "         [-3.3585, -3.1441, -2.7598,  ..., -2.0113, -2.6570, -1.1304],\n",
      "         [-3.4533, -3.2679, -2.8341,  ..., -2.2472, -2.7437, -1.1119]],\n",
      "\n",
      "        [[-4.0644, -3.6978, -3.3111,  ..., -3.2432, -2.3086, -0.2080],\n",
      "         [-6.9130, -6.4617, -6.5527,  ..., -3.9480, -2.0484, -0.7945],\n",
      "         [-7.9079, -7.7355, -7.8096,  ..., -1.0038, -0.8844,  1.4440],\n",
      "         ...,\n",
      "         [-2.3460, -2.9271, -2.4938,  ..., -0.2255, -0.3848,  0.6337],\n",
      "         [-2.2218, -2.8075, -2.4616,  ..., -0.0534, -0.4461,  0.6577],\n",
      "         [-2.3141, -2.9039, -2.4981,  ..., -0.1686, -0.4085,  0.6562]]],\n",
      "       grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 8.1262e-02, -1.3024e-02,  2.6007e-02,  ...,  3.8052e-02,\n",
      "          -5.7753e-02,  3.4088e-02],\n",
      "         [-3.3247e-02, -5.0434e-01,  3.8435e-01,  ..., -5.3380e-01,\n",
      "          -4.0373e-02, -2.7423e-01],\n",
      "         [-3.7588e-01,  1.2239e-01, -3.2900e-01,  ..., -3.3178e-01,\n",
      "           6.0873e-02,  1.2719e-01],\n",
      "         ...,\n",
      "         [ 3.3087e-02, -2.2870e-02,  2.2990e-02,  ...,  6.2797e-02,\n",
      "          -7.3525e-02,  2.0508e-02],\n",
      "         [ 8.5203e-02,  5.3053e-02,  3.5280e-02,  ..., -1.8947e-04,\n",
      "           8.8120e-02,  3.3872e-03],\n",
      "         [ 1.5385e-01,  2.0163e-01,  5.3718e-02,  ...,  1.3203e-01,\n",
      "          -1.3077e-01, -2.1896e-01]],\n",
      "\n",
      "        [[ 5.3141e-02,  4.7265e-03, -8.0484e-03,  ...,  5.1506e-02,\n",
      "          -1.3876e-02,  1.7657e-02],\n",
      "         [-3.4230e-02, -1.7473e-01,  4.8354e-01,  ..., -3.0285e-01,\n",
      "           8.8652e-02, -1.2946e-01],\n",
      "         [-4.0660e-01,  3.7206e-01,  2.3251e-01,  ..., -1.7349e-01,\n",
      "          -1.3090e-01, -2.5425e-01],\n",
      "         ...,\n",
      "         [ 4.4818e-02,  1.0542e-01,  6.5800e-03,  ...,  9.3055e-03,\n",
      "           1.5512e-01, -3.6989e-03],\n",
      "         [-1.6066e-01,  3.8552e-01,  5.1984e-03,  ...,  1.2133e-01,\n",
      "          -1.1595e-01,  7.5626e-02],\n",
      "         [-2.4083e-01,  3.3788e-01,  1.6331e-01,  ...,  8.2613e-02,\n",
      "           3.1001e-01, -4.3775e-01]],\n",
      "\n",
      "        [[ 6.8378e-02,  1.7749e-03, -6.2301e-03,  ...,  4.4853e-02,\n",
      "          -1.0987e-03,  1.5905e-02],\n",
      "         [-2.3284e-01, -2.0614e-01, -4.5688e-03,  ..., -3.5825e-01,\n",
      "          -5.7957e-01,  9.9387e-02],\n",
      "         [-7.9353e-02,  2.7519e-03, -5.7095e-01,  ...,  3.9126e-01,\n",
      "           1.0337e-01,  4.5679e-01],\n",
      "         ...,\n",
      "         [ 5.9477e-02,  9.2449e-02,  7.6147e-03,  ...,  3.2100e-03,\n",
      "           1.5385e-01, -2.3427e-04],\n",
      "         [ 4.7960e-01,  2.6435e-02,  2.8268e-01,  ..., -1.3175e-01,\n",
      "           1.1520e-01, -3.0357e-01],\n",
      "         [ 3.5495e-01,  7.4669e-02,  4.2451e-01,  ..., -4.1093e-02,\n",
      "           9.5622e-02, -4.4799e-01]],\n",
      "\n",
      "        [[ 7.3254e-02,  1.4890e-02, -4.2012e-03,  ...,  2.0587e-02,\n",
      "           3.8574e-03,  1.7662e-02],\n",
      "         [-7.7675e-02, -1.6268e-01,  6.6884e-01,  ...,  1.2397e-01,\n",
      "           3.4252e-02,  8.5194e-02],\n",
      "         [-3.6028e-01,  5.9519e-02,  4.5464e-01,  ..., -7.1071e-02,\n",
      "          -1.9539e-01,  2.9511e-02],\n",
      "         ...,\n",
      "         [ 6.0462e-02,  8.8657e-02,  7.7746e-03,  ..., -1.3341e-02,\n",
      "           1.3456e-01, -6.3312e-03],\n",
      "         [ 5.7830e-02,  8.4196e-02,  8.7782e-03,  ..., -1.4354e-02,\n",
      "           1.3272e-01, -4.0303e-03],\n",
      "         [ 5.1877e-01, -1.1525e-01,  4.1337e-01,  ..., -6.4690e-02,\n",
      "           1.4548e-01,  2.9702e-01]]], grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch) # 验证前向传播\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:16:36.348196500Z",
     "start_time": "2024-06-26T04:16:33.704420600Z"
    }
   },
   "id": "b4f8d6e709d697f6"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 将预测的 id 转换为 token\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 将标签转换为 token\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 去掉解码后的空格\n",
    "    decoded_preds = [\"\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # 计算 ROUGE\n",
    "    rouge = lawrouge.Rouge()\n",
    "    result = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "    result = {\n",
    "        'rouge-1': result['rouge-1']['f'],\n",
    "        'rouge-2': result['rouge-2']['f'],\n",
    "        'rouge-l': result['rouge-l']['f']\n",
    "    }\n",
    "    \n",
    "    # 将结果转换为百分比\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:17:07.682771200Z",
     "start_time": "2024-06-26T04:17:07.662229800Z"
    }
   },
   "id": "3b80e0089ab6de9b"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': 72.13622241177428, 'rouge-2': 43.52941126668205, 'rouge-l': 72.13622241177428}\n"
     ]
    }
   ],
   "source": [
    "# 示例数据 -> 是一个list，因为函数中使用了batch_decode\n",
    "predictions = ['我今天的午餐是牛肉饼', '我明天的晚餐是汉堡']\n",
    "targets = ['我今天中午吃牛肉饼', '我明天晚上吃汉堡']\n",
    "\n",
    "# 将示例数据进行tokenize\n",
    "predictions_tokenized = tokenizer(predictions, max_length=max_target_length, padding=True, truncation=True, return_tensors='pt')\n",
    "targets_tokenized = tokenizer(targets, max_length=max_target_length, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 模拟 eval_pred 格式\n",
    "eval_pred = (predictions_tokenized['input_ids'], targets_tokenized['input_ids'])\n",
    "\n",
    "# 计算评估指标\n",
    "results = compute_metrics(eval_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:19:01.167047300Z",
     "start_time": "2024-06-26T04:19:01.139079400Z"
    }
   },
   "id": "6325c6351e9553a2"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/content/results\", # 模型保存路径\n",
    "    num_train_epochs=epochs,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.0001,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    logging_steps=2000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,  # 设置评估步数\n",
    "    save_steps=2000,  # 设置保存步数\n",
    "    save_total_limit=3,\n",
    "    generation_max_length=max_target_length, # 生成的最大长度\n",
    "    generation_num_beams=3, # beam search -> 1 is greedy search\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge-1\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:20:53.868298Z",
     "start_time": "2024-06-26T04:20:53.773701Z"
    }
   },
   "id": "8a813c103d0a3a0b"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:20:56.752289Z",
     "start_time": "2024-06-26T04:20:55.883305700Z"
    }
   },
   "id": "e7812a27d217f290"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb846bd74dbbf89c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 打印验证集上的结果\n",
    "print(trainer.evaluate(tokenized_datasets[\"validation\"]))\n",
    "\n",
    "# 打印测试集上的结果\n",
    "print(trainer.evaluate(tokenized_datasets[\"test\"]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266271a1a30c04c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 保存最终模型\n",
    "trainer.save_model(\"results/best\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60908959beab86bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
